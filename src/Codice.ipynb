{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "\n",
    "def load_mat_file(path):\n",
    "    \"\"\"Load a .mat file and return the loaded data.\"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    return data\n",
    "\n",
    "# Function to save figures\n",
    "def save_figure(fig, filename, title=None):\n",
    "    directory = os.path.join(base_dir, \"Thesis Draft\", \"figures\")\n",
    "    report_file = os.path.join(base_dir, \"Thesis Draft\", \"reports\", \"report1.tex\")\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    fig_path = os.path.join(directory, filename)\n",
    "    fig.savefig(fig_path)\n",
    "\n",
    "    if title is None:\n",
    "        # Remove the file extension\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        # Replace underscores with spaces and capitalize each word\n",
    "        title = base_name.replace('_', ' ').title()\n",
    "\n",
    "    # Check if the figure path is already in the file\n",
    "    fig_include_str = fig_path.replace('\\\\', '/')\n",
    "    with open(report_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        if fig_include_str in content:\n",
    "            return\n",
    "\n",
    "    # Add the figure to the summary file\n",
    "    with open(report_file, 'a') as f:\n",
    "        f.write(\"\\\\begin{figure}[h]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\includegraphics[width=0.8\\\\textwidth]{{{}}}\\n\".format(fig_path.replace('\\\\', '/')))\n",
    "        f.write(\"\\\\caption{{{}}}\\n\".format(title))\n",
    "        f.write(\"\\\\end{figure}\\n\\n\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Print the type and shape of the matrices \\nfor key, matrix in matrices.items():\\n    print(f\"{key}: Type: {type(matrix)}, Shape: {matrix.shape}\") \\n    # sc_ya: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 101)\\n    # fc_ya: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 101)\\n    # sc_oa: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 78)\\n    # fc_oa: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 78)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load connectivity data from .mat file\n",
    "data_path = os.path.join(base_dir, \"scfc_schaefer100_ya_oa\", \"scfc_schaefer100_ya_oa.mat\")\n",
    "data = load_mat_file(data_path)\n",
    "\n",
    "\"\"\"\n",
    "# Print data type\n",
    "print(\"Type of data:\", type(data)) # <class 'dict'>\n",
    "\n",
    "# Print the keys in data\n",
    "if isinstance(data, dict):\n",
    "    print(\"Keys in data:\", data.keys()) # dict_keys(['__header__', '__version__', '__globals__', 'data'])\n",
    "\n",
    "    # Print the overview of the keys\n",
    "    for key in data.keys():\n",
    "        print(f\"Overview of data[{key}]: Type: {type(data[key])}, Shape/Length: {np.shape(data[key]) if hasattr(data[key], 'shape') else len(data[key])}\") # data[data]: Type: <class 'numpy.ndarray'>, Shape/Length: (1, 1) \n",
    "\"\"\"\n",
    "\n",
    "# Extract the content of the 'data' key\n",
    "data_content = data['data'][0,0] \n",
    "\n",
    "\"\"\"\n",
    "# Print the type of data content\n",
    "print(\"Type of data_content:\", type(data_content)) # <class 'numpy.void'>\n",
    "\n",
    "# Print the fields in sc_content and fc_content\n",
    "print(\"data_content fields:\", data_content.dtype.names) # ('sc_ya', 'fc_ya', 'sc_oa', 'fc_oa', 'info')\n",
    "\"\"\"\n",
    "\n",
    "# Extract the connectivity matrices and store them in a dictionary\n",
    "matrices = {}\n",
    "\n",
    "matrices['sc_ya'] = np.array(data_content['sc_ya'])\n",
    "matrices['fc_ya'] = np.array(data_content['fc_ya'])\n",
    "matrices['sc_oa'] = np.array(data_content['sc_oa'])\n",
    "matrices['fc_oa'] = np.array(data_content['fc_oa'])\n",
    "\n",
    "\"\"\"\n",
    "# Print the type and shape of the matrices \n",
    "for key, matrix in matrices.items():\n",
    "    print(f\"{key}: Type: {type(matrix)}, Shape: {matrix.shape}\") \n",
    "    # sc_ya: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 101)\n",
    "    # fc_ya: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 101)\n",
    "    # sc_oa: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 78)\n",
    "    # fc_oa: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 78)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef check_properties(matrices):\\n    for key, matrix in matrices.items():\\n        age_group, matrix_type = key.split(\\'_\\')\\n        num_subjects = matrix.shape[2]\\n\\n        for i in range(num_subjects):\\n            symmetric = check_symetric(matrix[:, :, i])\\n            zero_diagonal = check_zero_diagonal(matrix[:, :, i])\\n            correct_shape = check_dimensions(matrix[:, :, i], (100, 100))\\n            \\n            if not symmetric or not zero_diagonal or not correct_shape:\\n                print(f\"{matrix_type.upper()} {age_group.capitalize()} Subject {i+1}:\")\\n                if not symmetric:\\n                    print(\" Not Symmetric \")\\n                if not zero_diagonal:\\n                    print(\" Diagonal is not Zero \")\\n                if not correct_shape:\\n                    print(\" Incorrect Shape \")\\n\\n# Check properties of matrices\\n\\ncheck_properties(matrices)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check feasibility of the matrices\n",
    "\n",
    "def check_symetric(matrix):\n",
    "    \"\"\"Check if a matrix is symmetric.\"\"\"\n",
    "    return np.allclose(matrix, matrix.T)\n",
    "\n",
    "def check_zero_diagonal(matrix):\n",
    "    \"\"\"Check if the diagonal of a matrix is zero.\"\"\"\n",
    "    return np.allclose(np.diag(matrix), 0)\n",
    "\n",
    "def check_dimensions(matrix, expected_shape=(100, 100)):\n",
    "    \"\"\"Check if the matrix has the expected shape.\"\"\"\n",
    "    return matrix.shape == expected_shape\n",
    "\n",
    "# Check the properties of the matrices\n",
    "\"\"\"\n",
    "def check_properties(matrices):\n",
    "    for key, matrix in matrices.items():\n",
    "        age_group, matrix_type = key.split('_')\n",
    "        num_subjects = matrix.shape[2]\n",
    "\n",
    "        for i in range(num_subjects):\n",
    "            symmetric = check_symetric(matrix[:, :, i])\n",
    "            zero_diagonal = check_zero_diagonal(matrix[:, :, i])\n",
    "            correct_shape = check_dimensions(matrix[:, :, i], (100, 100))\n",
    "            \n",
    "            if not symmetric or not zero_diagonal or not correct_shape:\n",
    "                print(f\"{matrix_type.upper()} {age_group.capitalize()} Subject {i+1}:\")\n",
    "                if not symmetric:\n",
    "                    print(\" Not Symmetric \")\n",
    "                if not zero_diagonal:\n",
    "                    print(\" Diagonal is not Zero \")\n",
    "                if not correct_shape:\n",
    "                    print(\" Incorrect Shape \")\n",
    "\n",
    "# Check properties of matrices\n",
    "\n",
    "check_properties(matrices)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing?\n",
    "\n",
    "\n",
    "# Convert matrices to graphs\n",
    "def matrix_to_graph(matrix):\n",
    "    \"\"\"Convert a matrix to a graph.\"\"\"\n",
    "    # matrix = np.array(matrix)\n",
    "    graph = nx.from_numpy_array(matrix) \n",
    "    return graph\n",
    "\n",
    "\n",
    "graphs = {}\n",
    "\n",
    "for key, matrix in matrices.items():\n",
    "    num_subjects = matrix.shape[2]\n",
    "    graphs[key] = [matrix_to_graph(matrix[:, :, i]) for i in range(num_subjects)] #list of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graphs\n",
    "\n",
    "# plot node degree histogram and distribution from https://github.com/gordicaleksa/pytorch-GAT/blob/main/The%20Annotated%20GAT%20(Cora).ipynb\n",
    "\n",
    "def plot_graph_on_axis(graph, ax, title, pos=None, partition=None):\n",
    "    \"\"\"Plot a graph on a given axis with node sizes proportional to the degree and edge widths proportional to the edge weights. \n",
    "        If partition is provided, nodes are colored by their community.\"\"\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Extract edge weights\n",
    "    edge_weights = [graph[u][v]['weight'] for u, v in graph.edges()]\n",
    "    # Apply min-max normalization for edge thickness\n",
    "    min_weight = min(edge_weights)\n",
    "    max_weight = max(edge_weights)\n",
    "    if min_weight != max_weight:  # Avoid division by zero\n",
    "        edge_weights = [(w - min_weight) / (max_weight - min_weight) for w in edge_weights]\n",
    "    else:\n",
    "        edge_weights = [1 for _ in edge_weights]  # If all weights are the same, set them to 1\n",
    "\n",
    "    # Calculate the degree of each node for node size\n",
    "    degrees = dict(graph.degree())\n",
    "    # Normalize the degrees to the range 0-1\n",
    "    max_degree = max(degrees.values())\n",
    "    min_degree = min(degrees.values())\n",
    "    normalized_degrees = {node: (degree - min_degree) / (max_degree - min_degree) for node, degree in degrees.items()}\n",
    "    # Set node sizes based on normalized degrees\n",
    "    node_sizes = [(normalized_degrees[node] + 0.1) * 5 for node in graph.nodes()]  \n",
    "\n",
    "    if partition:\n",
    "        # If partition is provided, color nodes by their community\n",
    "        cmap = plt.get_cmap('viridis', max(partition.values()) + 1)\n",
    "        node_color = list(partition.values())\n",
    "    else:\n",
    "        node_color = 'steelblue'\n",
    "    \n",
    "    # Draw the graph\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(graph, seed=42)\n",
    "    nx.draw(graph, pos, ax=ax, node_size=node_sizes, with_labels=False, node_color=node_color, cmap=cmap if partition else None, edge_color='gray', width=edge_weights)\n",
    "\n",
    "    return pos\n",
    "\n",
    "\n",
    "def plot_and_save_graph(graphs, filename=None, positions=None, partitions=None, status=\"Original\", with_communities=False, num_subjects=1):\n",
    "    \"\"\"\n",
    "    Plot and save graphs for different age groups on a grid of subplots.\n",
    "    \n",
    "    Parameters:\n",
    "    - graphs: Dictionary containing NetworkX graphs for each age group and matrix type\n",
    "    - filename: name.png\n",
    "    - positions: List of positions for the graphs\n",
    "    - partitions: Dictionary containing partitions for the graphs of each age group and matrix type\n",
    "    - status: 'Original', 'Preprocessed', or 'Louvain Preprocessed'\n",
    "    - num_subjects: Number of subjects to plot (default is 1)\n",
    "    \"\"\"\n",
    "    community_text = \" with Louvain Communities\" if with_communities else \"\"\n",
    "    title = f'{status} Graphs{community_text}'\n",
    "    fig, axs = plt.subplots(4, num_subjects, figsize=(15, 9))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    if positions is None:\n",
    "        positions = {}\n",
    "    \n",
    "    if partitions is None:\n",
    "        partitions = {key: [None] * num_subjects for key in graphs.keys()}\n",
    "\n",
    "    # TODO\n",
    "    \"\"\"for j, (key, graph_list) in enumerate(graphs.items()):\n",
    "        age_group, matrix_type = key.split('_')\n",
    "        title_prefix = matrix_type.upper()\n",
    "        for i in range(min(num_subjects, len(graph_list))):\n",
    "            if len(positions.get(key, [])) > i:\n",
    "                pos = positions[key][i]\n",
    "                plot_graph_on_axis(graph_list[i], axs[j, i], f\"{key.upper()} Graph {i+1}\", pos, partitions[key][i] if partitions[key] else None)\n",
    "            else:\n",
    "                pos = plot_graph_on_axis(graph_list[i], axs[j, i], f\"{key.upper()} Graph {i+1}\", partition=partitions[key][i] if partitions[key] else None)\n",
    "                if key not in positions:\n",
    "                    positions[key] = []\n",
    "                positions[key].append(pos)\"\"\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    if filename:\n",
    "        save_figure(fig, filename, title)\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "\n",
    "# plot_and_save_graph(graphs, filename='graphs.png', status=' ', num_subjects=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT for aging biomarker identification\n",
    "\n",
    "# combine structural and functional graphs in a single graph\n",
    "\n",
    "import torch\n",
    "import scipy.stats\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv, TopKPooling, global_mean_pool\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(f'graphs_sc shape: {len(graphs_sc)}, type: {type(graphs_sc)}')\\nprint(f'labels_sc shape: {len(labels_sc)}, type: {type(labels_sc)}')\\n\\nprint(f'graphs_fc_sc shape: {len(graphs_fc_sc)}, type: {type(graphs_fc_sc)}')\\nprint(f'labels_fc_sc shape: {len(labels_fc_sc)}, type: {type(labels_fc_sc)}')\\n\\n# print(f'graphs_fc shape: {len(graphs_fc)}, type: {type(graphs_fc)}')\\n# print(f'labels_fc shape: {len(labels_fc)}, type: {type(labels_fc)}')\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topological measures that significantly change with age across the lifespan\n",
    "# different among structural and functional networks?\n",
    "\n",
    "# functional connectivity topological measures\n",
    "# from Cao 2014 \n",
    "# Local efficiency (inverted U shape) (sex differences only in global?)\n",
    "# Modularity (decrease linearly) ( without global signal regression modularity failed to detect age effects)\n",
    "# Mean connectivity strength (negative quadratic trajectories)\n",
    "# Normalized Rich Club coefficients (inverse U shape over a range of hub thresholds) (no significant sex differences)\n",
    "# Regional Functional Connectivity Strength (rFCS) (Age-related linear and quadratic changes both positive and negative) (male higher connectivity strength in some areas)\n",
    "# participation coefficient?\n",
    "# centrality measures? (sub-graph centrality)\n",
    "\n",
    "# structural connectivity topological measures\n",
    "\n",
    "# participation coefficient\n",
    "# contribution to modularity\n",
    "\n",
    "# matching index\n",
    "\n",
    "\n",
    "def calculate_node_features(matrix):\n",
    "    \"\"\"Calculate node features from a connectivity matrix.\n",
    "    Features are from connectivity statistics: mean, standard deviation, kurtosis, skewness,\n",
    "    and degree centrality of the node's connectivity vector to all the other nodes (Yang 2019).\n",
    "    \"\"\"\n",
    "    node_features = []\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "\n",
    "\n",
    "        # Functional features from connectivity statistics\n",
    "        connections = matrix[i, :]\n",
    "        connections = connections.flatten()\n",
    "        mean = connections.mean().item()\n",
    "        std = connections.std().item()\n",
    "        skew = scipy.stats.skew(connections).item()\n",
    "        kurtosis = scipy.stats.kurtosis(connections).item()\n",
    "        \n",
    "        # Calculate degree centrality\n",
    "        \n",
    "        node_features.append([mean, std, skew, kurtosis])\n",
    "    \n",
    "    # Convert the list of lists to a numpy array before converting to a tensor\n",
    "    node_features = np.array(node_features)\n",
    "    return torch.tensor(node_features, dtype=torch.float32)\n",
    "\n",
    "\n",
    "#TODO: Capire se c'è variabilità nelle feature (valori medi, istogramma)\n",
    "# dimensionality reduction (PCA, autoencoders) or feature selections to decrease the number of features?\n",
    "\n",
    "def combined_graph(matrix, feature_tensor=None, feature_type='random'):\n",
    "    \"\"\"Combine a connectivity matrix and a feature tensor into a single graph.\n",
    "    Graphs are constructed from the connectivity matrix and the node features \n",
    "    consist of the provided feature tensor. If feature_tensor is not provided,\n",
    "    the user can choose between using a random tensor or an identity tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    - matrix: The connectivity matrix.\n",
    "    - feature_tensor: The tensor containing node features. If None, feature_type is used.\n",
    "    - feature_type: The type of tensor to use if feature_tensor is None. Options are 'random' or 'identity'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turn matrix into torch tensor\n",
    "    tensor_matrix = torch.tensor(matrix, dtype=torch.float)\n",
    "\n",
    "    # Initialize empty list for storing graph data objects\n",
    "    graph_list = []\n",
    "\n",
    "    # Determine the number of subjects and nodes\n",
    "    num_subjects = tensor_matrix.shape[2]\n",
    "    num_nodes = tensor_matrix.shape[0]\n",
    "\n",
    "    # Generate feature tensor if not provided\n",
    "    if feature_tensor is None:\n",
    "        if feature_type == 'random':\n",
    "            feature_tensor = torch.rand((num_nodes, 4), dtype=torch.float32)\n",
    "        elif feature_type == 'identity':\n",
    "            feature_tensor = torch.eye(num_nodes, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid feature_type. Choose 'random' or 'identity'.\")\n",
    "\n",
    "    # Iterate over the third dimension of tensor_matrix to fill them with node features for each subject\n",
    "    for i in range(num_subjects):\n",
    "        # Set edges for the graph as in tensor_matrix\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        for j in range(num_nodes):\n",
    "            for k in range(j+1, num_nodes):\n",
    "                weight = tensor_matrix[j, k, i]\n",
    "                # Remove null edges? (Yang doesn't)  \n",
    "                # Add both directions since undirected?\n",
    "                if weight != 0:  # Remove null edges\n",
    "                    edges.append([j, k])\n",
    "                    edges.append([k, j])\n",
    "                    edge_weights.append(weight)\n",
    "                    edge_weights.append(weight)\n",
    "\n",
    "        # Convert graph edges in form of torch tensor of size (2, num_edges)\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() \n",
    "\n",
    "        # Convert graph edge weights in form of torch tensor \n",
    "        edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "\n",
    "        # Create graph data object\n",
    "        data = Data(x=feature_tensor, edge_index=edge_index, edge_attr=edge_weights)\n",
    "        \n",
    "        # Append the graph data object to the list\n",
    "        graph_list.append(data)\n",
    "\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "# Calculate node features for both structural and functional matrices\n",
    "features = {\n",
    "    'sc_ya': calculate_node_features(matrices['sc_ya']),\n",
    "    'sc_oa': calculate_node_features(matrices['sc_oa']),\n",
    "    'fc_ya': calculate_node_features(matrices['fc_ya']),\n",
    "    'fc_oa': calculate_node_features(matrices['fc_oa']),\n",
    "    'combined_ya': torch.cat((calculate_node_features(matrices['sc_ya']), calculate_node_features(matrices['fc_ya'])), dim=1),\n",
    "    'combined_oa': torch.cat((calculate_node_features(matrices['sc_oa']), calculate_node_features(matrices['fc_oa'])), dim=1),\n",
    "}\n",
    "\n",
    "\n",
    "# Each element is a list of graph data objects\n",
    "#TODO: stack functional and structural features together?\n",
    "#TODO: using multi-edge connections instead of separate modalities?\n",
    "combined_graphs = {\n",
    "    'sc_ya': combined_graph(matrices['sc_ya'], feature_tensor=features['sc_ya']),\n",
    "    'sc_oa': combined_graph(matrices['sc_oa'], feature_tensor=features['sc_oa']),\n",
    "    #'fc_ya': combined_graph(matrices['fc_ya'], feature_tensor=features['fc_ya']),\n",
    "    #'fc_oa': combined_graph(matrices['fc_oa'], feature_tensor=features['fc_oa']),\n",
    "    'fc_sc_ya': combined_graph(matrices['fc_ya'], feature_tensor=features['combined_ya']),\n",
    "    'fc_sc_oa': combined_graph(matrices['fc_oa'], feature_tensor=features['combined_oa']),\n",
    "}\n",
    "\n",
    "# Print the len of the combined graphs\n",
    "\"\"\"\n",
    "for key, graph_list in combined_graphs.items():\n",
    "    print(f\"{key}: {len(graph_list)}\")\n",
    "\"\"\"\n",
    "\n",
    "# check number of nodes and non-zero edges?\n",
    "\n",
    "\n",
    "# Split dataset_sc into separate lists of graphs and labels\n",
    "graphs_sc = [graph for label in ['sc_ya', 'sc_oa'] for graph in combined_graphs[label]]\n",
    "labels_sc = [label for label in ['sc_ya', 'sc_oa'] for _ in combined_graphs[label]]\n",
    "\n",
    "# Split dataset_fc_sc into separate lists of graphs and labels\n",
    "graphs_fc_sc = [graph for label in ['fc_sc_ya', 'fc_sc_oa'] for graph in combined_graphs[label]]\n",
    "labels_fc_sc = [label for label in ['fc_sc_ya', 'fc_sc_oa'] for _ in combined_graphs[label]]\n",
    "\n",
    "# graphs_fc = [graph for label in ['fc_ya', 'fc_oa'] for graph_list in combined_graphs[label]]\n",
    "# labels_fc = [label for label in ['fc_ya', 'fc_oa'] for graph_list in combined_graphs[label]]\n",
    "\n",
    "#Print the shapes of graphs and labels\n",
    "\"\"\"\n",
    "print(f'graphs_sc shape: {len(graphs_sc)}, type: {type(graphs_sc)}')\n",
    "print(f'labels_sc shape: {len(labels_sc)}, type: {type(labels_sc)}')\n",
    "\n",
    "print(f'graphs_fc_sc shape: {len(graphs_fc_sc)}, type: {type(graphs_fc_sc)}')\n",
    "print(f'labels_fc_sc shape: {len(labels_fc_sc)}, type: {type(labels_fc_sc)}')\n",
    "\n",
    "# print(f'graphs_fc shape: {len(graphs_fc)}, type: {type(graphs_fc)}')\n",
    "# print(f'labels_fc shape: {len(labels_fc)}, type: {type(labels_fc)}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT model\n",
    "# Interpretable graph classification\n",
    "# Captum for interpretability?\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gat.py\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=1): #hidden_channels\n",
    "        #super(GAT, self).__init__()\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=heads, concat=True) # dropout=0.6)\n",
    "        self.classifier = nn.Linear(out_channels*heads, 2)  # Adjust output size based on concatenation (dataset.num_classes)\n",
    "        # fully connected layer\n",
    "        # On the Pubmed dataset, use `heads` output heads in `conv2`.\n",
    "        #self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1,concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.classifier(x)\n",
    "        x= F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = GAT(in_channels=4, out_channels=8, heads = 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, labels in loader:\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(f'train_data_sc len: {len(train_data_sc)}, type: {type(train_data_sc)}') \\nprint(f'test_data_sc len: {len(test_data_sc)}, type: {type(test_data_sc)}')\\nprint(f'train_labels_sc len: {len(train_labels_sc)}, type: {type(train_labels_sc)}')\\nprint(f'test_labels_sc len: {len(test_labels_sc)}, type: {type(test_labels_sc)}')\\n\\nprint(f'train_data_fc_sc len: {len(train_data_fc_sc)}, type: {type(train_data_fc_sc)}')\\nprint(f'test_data_fc_sc len: {len(test_data_fc_sc)}, type: {type(test_data_fc_sc)}')\\nprint(f'train_labels_fc_sc len: {len(train_labels_fc_sc)}, type: {type(train_labels_fc_sc)}')\\nprint(f'test_labels_fc_sc len: {len(test_labels_fc_sc)}')\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data with stratification\n",
    "train_data_sc, test_data_sc, train_labels_sc, test_labels_sc = train_test_split(graphs_sc, labels_sc, test_size=0.3, random_state=42, stratify=labels_sc)\n",
    "train_data_fc_sc, test_data_fc_sc, train_labels_fc_sc, test_labels_fc_sc = train_test_split(graphs_fc_sc, labels_fc_sc, test_size=0.3, random_state=42, stratify=labels_fc_sc)\n",
    "# train_data_fc, test_data_fc, train_labels_fc, test_labels_fc = train_test_split(graphs_fc, labels_fc, test_size=0.3, random_state=42, stratify=labels_fc)\n",
    "\n",
    "# Print the shapes of the split data\n",
    "\"\"\"\n",
    "print(f'train_data_sc len: {len(train_data_sc)}, type: {type(train_data_sc)}') \n",
    "print(f'test_data_sc len: {len(test_data_sc)}, type: {type(test_data_sc)}')\n",
    "print(f'train_labels_sc len: {len(train_labels_sc)}, type: {type(train_labels_sc)}')\n",
    "print(f'test_labels_sc len: {len(test_labels_sc)}, type: {type(test_labels_sc)}')\n",
    "\n",
    "print(f'train_data_fc_sc len: {len(train_data_fc_sc)}, type: {type(train_data_fc_sc)}')\n",
    "print(f'test_data_fc_sc len: {len(test_data_fc_sc)}, type: {type(test_data_fc_sc)}')\n",
    "print(f'train_labels_fc_sc len: {len(train_labels_fc_sc)}, type: {type(train_labels_fc_sc)}')\n",
    "print(f'test_labels_fc_sc len: {len(test_labels_fc_sc)}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(f'train_dataset_sc len: {len(train_dataset_sc)}')\\nprint(f'test_dataset_sc len: {len(test_dataset_sc)}')\\n\\nprint(f'train_dataset_fc_sc len: {len(train_dataset_fc_sc)}')\\nprint(f'test_dataset_fc_sc len: {len(test_dataset_fc_sc)}')\\n\\n# print(f'train_dataset_fc len: {len(train_dataset_fc)}')\\n# print(f'test_dataset_fc len: {len(test_dataset_fc)}')\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PyTorch dataset as a subclass of torch.utils.data.Dataset\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs, labels):\n",
    "        self.graphs = graphs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return graph, label\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset_sc = GraphDataset(train_data_sc, train_labels_sc)\n",
    "test_dataset_sc = GraphDataset(test_data_sc, test_labels_sc)\n",
    "\n",
    "train_dataset_fc_sc = GraphDataset(train_data_fc_sc, train_labels_fc_sc)\n",
    "test_dataset_fc_sc = GraphDataset(test_data_fc_sc, test_labels_fc_sc)\n",
    "\n",
    "# train_dataset_fc = GraphDataset(train_data_fc, train_labels_fc)\n",
    "# test_dataset_fc = GraphDataset(test_data_fc, test_labels_fc)\n",
    "\n",
    "# Print the length of the datasets\n",
    "\"\"\"\n",
    "print(f'train_dataset_sc len: {len(train_dataset_sc)}')\n",
    "print(f'test_dataset_sc len: {len(test_dataset_sc)}')\n",
    "\n",
    "print(f'train_dataset_fc_sc len: {len(train_dataset_fc_sc)}')\n",
    "print(f'test_dataset_fc_sc len: {len(test_dataset_fc_sc)}')\n",
    "\n",
    "# print(f'train_dataset_fc len: {len(train_dataset_fc)}')\n",
    "# print(f'test_dataset_fc len: {len(test_dataset_fc)}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_sc = DataLoader(train_dataset_sc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_sc = DataLoader(test_dataset_sc, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_fc_sc = DataLoader(train_dataset_fc_sc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_fc_sc = DataLoader(test_dataset_fc_sc, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train_loader_fc = DataLoader(train_dataset_fc, batch_size=batch_size, shuffle=True)\n",
    "# test_loader_fc = DataLoader(test_dataset_fc, batch_size=batch_size, shuffle=False)ù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m best_test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_sc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m test(test_loader_sc)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_accuracy \u001b[38;5;241m>\u001b[39m best_test_acc:\n",
      "Cell \u001b[1;32mIn[26], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader)\u001b[0m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     40\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m---> 41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\barbo\\Desktop\\thesis repo clone 2\\thesis2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\barbo\\Desktop\\thesis repo clone 2\\thesis2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\barbo\\Desktop\\thesis repo clone 2\\thesis2\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\barbo\\Desktop\\thesis repo clone 2\\thesis2\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'target' (position 2) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 20\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_loader_sc)\n",
    "    test_accuracy = test(test_loader_sc)\n",
    "    \n",
    "    if test_accuracy > best_test_acc:\n",
    "        best_test_acc = test_accuracy\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "print(f'Best Test Accuracy: {best_test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# This website has got some awesome visualizations check it out:\n",
    "# http://networkrepository.com/graphvis.php?d=./data/gsm50/labeled/cora.edges\n",
    "\n",
    "#https://towardsdatascience.com/large-graph-visualization-tools-and-approaches-2b8758a1cd59\n",
    "\n",
    "# igraph "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
