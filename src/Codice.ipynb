{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: c:\\Users\\barbo\\brain classifier repo\\brain_classifier\\src\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "\n",
    "\n",
    "print(\"Base directory: {}\".format(base_dir))\n",
    "\n",
    "def load_mat_file(path):\n",
    "    \"\"\"Load a .mat file and return the loaded data.\"\"\"\n",
    "    data = scipy.io.loadmat(path)\n",
    "    return data\n",
    "\n",
    "# Function to save figures\n",
    "def save_figure(fig, filename, title=None):\n",
    "    directory = os.path.join(base_dir, \"Thesis Draft\", \"figures\")\n",
    "    report_file = os.path.join(base_dir, \"Thesis Draft\", \"reports\", \"report1.tex\")\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    fig_path = os.path.join(directory, filename)\n",
    "    fig.savefig(fig_path)\n",
    "\n",
    "    if title is None:\n",
    "        # Remove the file extension\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        # Replace underscores with spaces and capitalize each word\n",
    "        title = base_name.replace('_', ' ').title()\n",
    "\n",
    "    # Check if the figure path is already in the file\n",
    "    fig_include_str = fig_path.replace('\\\\', '/')\n",
    "    with open(report_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        if fig_include_str in content:\n",
    "            return\n",
    "\n",
    "    # Add the figure to the summary file\n",
    "    with open(report_file, 'a') as f:\n",
    "        f.write(\"\\\\begin{figure}[h]\\n\")\n",
    "        f.write(\"\\\\centering\\n\")\n",
    "        f.write(\"\\\\includegraphics[width=0.8\\\\textwidth]{{{}}}\\n\".format(fig_path.replace('\\\\', '/')))\n",
    "        f.write(\"\\\\caption{{{}}}\\n\".format(title))\n",
    "        f.write(\"\\\\end{figure}\\n\\n\")\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor key, matrix in matrices.items():\\n    print(f\"{key}: Type: {type(matrix)}, Shape: {matrix.shape}\") \\n    # sc_ya: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 101)\\n    # fc_ya: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 101)\\n    # sc_oa: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 78)\\n    # fc_oa: Type: <class \\'numpy.ndarray\\'>, Shape: (100, 100, 78)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load connectivity data from .mat file\n",
    "data_path = os.path.join(base_dir, \"..\", \"data\", \"scfc_schaefer100_ya_oa.mat\")\n",
    "data = load_mat_file(data_path)\n",
    "\n",
    "\"\"\"\n",
    "# Print data type\n",
    "print(\"Type of data:\", type(data)) # <class 'dict'>\n",
    "\n",
    "# Print the keys in data\n",
    "if isinstance(data, dict):\n",
    "    print(\"Keys in data:\", data.keys()) # dict_keys(['__header__', '__version__', '__globals__', 'data'])\n",
    "\n",
    "    # Print the overview of the keys\n",
    "    for key in data.keys():\n",
    "        print(f\"Overview of data[{key}]: Type: {type(data[key])}, Shape/Length: {np.shape(data[key]) if hasattr(data[key], 'shape') else len(data[key])}\") # data[data]: Type: <class 'numpy.ndarray'>, Shape/Length: (1, 1) \n",
    "\"\"\"\n",
    "\n",
    "# Extract the content of the 'data' key\n",
    "data_content = data['data'][0,0] \n",
    "\n",
    "\"\"\"\n",
    "# Print the type of data content\n",
    "print(\"Type of data_content:\", type(data_content)) # <class 'numpy.void'>\n",
    "\n",
    "# Print the fields in sc_content and fc_content\n",
    "print(\"data_content fields:\", data_content.dtype.names) # ('sc_ya', 'fc_ya', 'sc_oa', 'fc_oa', 'info')\n",
    "\"\"\"\n",
    "\n",
    "# Extract the connectivity matrices and store them in a dictionary\n",
    "matrices = {}\n",
    "\n",
    "matrices['sc_ya'] = np.array(data_content['sc_ya'])\n",
    "matrices['fc_ya'] = np.array(data_content['fc_ya'])\n",
    "matrices['sc_oa'] = np.array(data_content['sc_oa'])\n",
    "matrices['fc_oa'] = np.array(data_content['fc_oa'])\n",
    "\n",
    "\n",
    "# Print the type and shape of the matrices \n",
    "\"\"\"\n",
    "for key, matrix in matrices.items():\n",
    "    print(f\"{key}: Type: {type(matrix)}, Shape: {matrix.shape}\") \n",
    "    # sc_ya: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 101)\n",
    "    # fc_ya: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 101)\n",
    "    # sc_oa: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 78)\n",
    "    # fc_oa: Type: <class 'numpy.ndarray'>, Shape: (100, 100, 78)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef check_properties(matrices):\\n    for key, matrix in matrices.items():\\n        age_group, matrix_type = key.split(\\'_\\')\\n        num_subjects = matrix.shape[2]\\n\\n        for i in range(num_subjects):\\n            symmetric = check_symetric(matrix[:, :, i])\\n            zero_diagonal = check_zero_diagonal(matrix[:, :, i])\\n            correct_shape = check_dimensions(matrix[:, :, i], (100, 100))\\n            \\n            if not symmetric or not zero_diagonal or not correct_shape:\\n                print(f\"{matrix_type.upper()} {age_group.capitalize()} Subject {i+1}:\")\\n                if not symmetric:\\n                    print(\" Not Symmetric \")\\n                if not zero_diagonal:\\n                    print(\" Diagonal is not Zero \")\\n                if not correct_shape:\\n                    print(\" Incorrect Shape \")\\n\\n# Check properties of matrices\\n\\ncheck_properties(matrices)\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check feasibility of the matrices\n",
    "\n",
    "def check_symetric(matrix):\n",
    "    \"\"\"Check if a matrix is symmetric.\"\"\"\n",
    "    return np.allclose(matrix, matrix.T)\n",
    "\n",
    "def check_zero_diagonal(matrix):\n",
    "    \"\"\"Check if the diagonal of a matrix is zero.\"\"\"\n",
    "    return np.allclose(np.diag(matrix), 0)\n",
    "\n",
    "def check_dimensions(matrix, expected_shape=(100, 100)):\n",
    "    \"\"\"Check if the matrix has the expected shape.\"\"\"\n",
    "    return matrix.shape == expected_shape\n",
    "\n",
    "# Check the properties of the matrices\n",
    "\"\"\"\n",
    "def check_properties(matrices):\n",
    "    for key, matrix in matrices.items():\n",
    "        age_group, matrix_type = key.split('_')\n",
    "        num_subjects = matrix.shape[2]\n",
    "\n",
    "        for i in range(num_subjects):\n",
    "            symmetric = check_symetric(matrix[:, :, i])\n",
    "            zero_diagonal = check_zero_diagonal(matrix[:, :, i])\n",
    "            correct_shape = check_dimensions(matrix[:, :, i], (100, 100))\n",
    "            \n",
    "            if not symmetric or not zero_diagonal or not correct_shape:\n",
    "                print(f\"{matrix_type.upper()} {age_group.capitalize()} Subject {i+1}:\")\n",
    "                if not symmetric:\n",
    "                    print(\" Not Symmetric \")\n",
    "                if not zero_diagonal:\n",
    "                    print(\" Diagonal is not Zero \")\n",
    "                if not correct_shape:\n",
    "                    print(\" Incorrect Shape \")\n",
    "\n",
    "# Check properties of matrices\n",
    "\n",
    "check_properties(matrices)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing?\n",
    "\n",
    "\n",
    "# Convert matrices to graphs\n",
    "def matrix_to_graph(matrix):\n",
    "    \"\"\"Convert a matrix to a graph.\"\"\"\n",
    "    # matrix = np.array(matrix)\n",
    "    graph = nx.from_numpy_array(matrix) \n",
    "    return graph\n",
    "\n",
    "\n",
    "graphs = {}\n",
    "\n",
    "for key, matrix in matrices.items():\n",
    "    num_subjects = matrix.shape[2]\n",
    "    graphs[key] = [matrix_to_graph(matrix[:, :, i]) for i in range(num_subjects)] #list of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graphs\n",
    "\n",
    "# plot node degree histogram and distribution from https://github.com/gordicaleksa/pytorch-GAT/blob/main/The%20Annotated%20GAT%20(Cora).ipynb\n",
    "\n",
    "def plot_graph_on_axis(graph, ax, title, pos=None, partition=None):\n",
    "    \"\"\"Plot a graph on a given axis with node sizes proportional to the degree and edge widths proportional to the edge weights. \n",
    "        If partition is provided, nodes are colored by their community.\"\"\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Extract edge weights\n",
    "    edge_weights = [graph[u][v]['weight'] for u, v in graph.edges()]\n",
    "    # Apply min-max normalization for edge thickness\n",
    "    min_weight = min(edge_weights)\n",
    "    max_weight = max(edge_weights)\n",
    "    if min_weight != max_weight:  # Avoid division by zero\n",
    "        edge_weights = [(w - min_weight) / (max_weight - min_weight) for w in edge_weights]\n",
    "    else:\n",
    "        edge_weights = [1 for _ in edge_weights]  # If all weights are the same, set them to 1\n",
    "\n",
    "    # Calculate the degree of each node for node size\n",
    "    degrees = dict(graph.degree())\n",
    "    # Normalize the degrees to the range 0-1\n",
    "    max_degree = max(degrees.values())\n",
    "    min_degree = min(degrees.values())\n",
    "    normalized_degrees = {node: (degree - min_degree) / (max_degree - min_degree) for node, degree in degrees.items()}\n",
    "    # Set node sizes based on normalized degrees\n",
    "    node_sizes = [(normalized_degrees[node] + 0.1) * 5 for node in graph.nodes()]  \n",
    "\n",
    "    if partition:\n",
    "        # If partition is provided, color nodes by their community\n",
    "        cmap = plt.get_cmap('viridis', max(partition.values()) + 1)\n",
    "        node_color = list(partition.values())\n",
    "    else:\n",
    "        node_color = 'steelblue'\n",
    "    \n",
    "    # Draw the graph\n",
    "    if pos is None:\n",
    "        pos = nx.spring_layout(graph, seed=42)\n",
    "    nx.draw(graph, pos, ax=ax, node_size=node_sizes, with_labels=False, node_color=node_color, cmap=cmap if partition else None, edge_color='gray', width=edge_weights)\n",
    "\n",
    "    return pos\n",
    "\n",
    "\n",
    "def plot_and_save_graph(graphs, filename=None, positions=None, partitions=None, status=\"Original\", with_communities=False, num_subjects=1):\n",
    "    \"\"\"\n",
    "    Plot and save graphs for different age groups on a grid of subplots.\n",
    "    \n",
    "    Parameters:\n",
    "    - graphs: Dictionary containing NetworkX graphs for each age group and matrix type\n",
    "    - filename: name.png\n",
    "    - positions: List of positions for the graphs\n",
    "    - partitions: Dictionary containing partitions for the graphs of each age group and matrix type\n",
    "    - status: 'Original', 'Preprocessed', or 'Louvain Preprocessed'\n",
    "    - num_subjects: Number of subjects to plot (default is 1)\n",
    "    \"\"\"\n",
    "    community_text = \" with Louvain Communities\" if with_communities else \"\"\n",
    "    title = f'{status} Graphs{community_text}'\n",
    "    fig, axs = plt.subplots(4, num_subjects, figsize=(15, 9))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    if positions is None:\n",
    "        positions = {}\n",
    "    \n",
    "    if partitions is None:\n",
    "        partitions = {key: [None] * num_subjects for key in graphs.keys()}\n",
    "\n",
    "    # TODO\n",
    "    \"\"\"for j, (key, graph_list) in enumerate(graphs.items()):\n",
    "        age_group, matrix_type = key.split('_')\n",
    "        title_prefix = matrix_type.upper()\n",
    "        for i in range(min(num_subjects, len(graph_list))):\n",
    "            if len(positions.get(key, [])) > i:\n",
    "                pos = positions[key][i]\n",
    "                plot_graph_on_axis(graph_list[i], axs[j, i], f\"{key.upper()} Graph {i+1}\", pos, partitions[key][i] if partitions[key] else None)\n",
    "            else:\n",
    "                pos = plot_graph_on_axis(graph_list[i], axs[j, i], f\"{key.upper()} Graph {i+1}\", partition=partitions[key][i] if partitions[key] else None)\n",
    "                if key not in positions:\n",
    "                    positions[key] = []\n",
    "                positions[key].append(pos)\"\"\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    if filename:\n",
    "        save_figure(fig, filename, title)\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "\n",
    "# plot_and_save_graph(graphs, filename='graphs.png', status=' ', num_subjects=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.stats\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.utils.data import Dataset, random_split, Subset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topological measures that significantly change with age across the lifespan\n",
    "# different among structural and functional networks?\n",
    "\n",
    "# functional connectivity topological measures\n",
    "# from Cao 2014 \n",
    "# Local efficiency (inverted U shape) (sex differences only in global?)\n",
    "# Modularity (decrease linearly) ( without global signal regression modularity failed to detect age effects)\n",
    "# Mean connectivity strength (negative quadratic trajectories)\n",
    "# Normalized Rich Club coefficients (inverse U shape over a range of hub thresholds) (no significant sex differences)\n",
    "# Regional Functional Connectivity Strength (rFCS) (Age-related linear and quadratic changes both positive and negative) (male higher connectivity strength in some areas)\n",
    "# participation coefficient?\n",
    "# centrality measures? (sub-graph centrality)\n",
    "\n",
    "# structural connectivity topological measures\n",
    "\n",
    "# participation coefficient\n",
    "# contribution to modularity\n",
    "\n",
    "# matching index\n",
    "\n",
    "\n",
    "def calculate_node_features(matrix):\n",
    "    \"\"\"Calculate node features from a connectivity matrix.\n",
    "    Features are from connectivity statistics: mean, standard deviation, kurtosis, skewness,\n",
    "    and degree centrality of the node's connectivity vector to all the other nodes (Yang 2019).\n",
    "    \"\"\"\n",
    "    node_features = []\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "\n",
    "        # Functional features from connectivity statistics\n",
    "        connections = matrix[i, :]\n",
    "        connections = connections.flatten()\n",
    "        mean = connections.mean()\n",
    "        std = connections.std()\n",
    "        skew = scipy.stats.skew(connections)\n",
    "        kurtosis = scipy.stats.kurtosis(connections)\n",
    "        \n",
    "        # Calculate degree centrality\n",
    "        \n",
    "        node_features.append([mean, std, skew, kurtosis])\n",
    "    \n",
    "    # Convert the list of lists to a numpy array before converting to a tensor\n",
    "    node_features = np.array(node_features)\n",
    "    return torch.tensor(node_features, dtype=torch.float32)\n",
    "\n",
    "\n",
    "#TODO: Capire se c'è variabilità nelle feature (valori medi, istogramma)\n",
    "# dimensionality reduction (PCA, autoencoders) or feature selections to decrease the number of features?\n",
    "\n",
    "def combined_graph(matrix, feature_tensor=None, feature_type='random'):\n",
    "    \"\"\"Combine a connectivity matrix and a feature tensor into a single graph.\n",
    "    Graphs are constructed from the connectivity matrix and the node features \n",
    "    consist of the provided feature tensor. If feature_tensor is not provided,\n",
    "    the user can choose between using a random tensor or an identity tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    - matrix: The connectivity matrix.\n",
    "    - feature_tensor: The tensor containing node features. If None, feature_type is used.\n",
    "    - feature_type: The type of tensor to use if feature_tensor is None. Options are 'random' or 'identity'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turn matrix into torch tensor\n",
    "    tensor_matrix = torch.tensor(matrix, dtype=torch.float)\n",
    "\n",
    "    # Initialize empty list for storing graph data objects\n",
    "    graph_list = []\n",
    "\n",
    "    # Determine the number of subjects and nodes\n",
    "    num_subjects = tensor_matrix.shape[2]\n",
    "    num_nodes = tensor_matrix.shape[0]\n",
    "\n",
    "    # Generate feature tensor if not provided\n",
    "    if feature_tensor is None:\n",
    "        if feature_type == 'random':\n",
    "            feature_tensor = torch.rand((num_nodes, 4), dtype=torch.float32)\n",
    "        elif feature_type == 'identity':\n",
    "            feature_tensor = torch.eye(num_nodes, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid feature_type. Choose 'random' or 'identity'.\")\n",
    "\n",
    "    # Iterate over the third dimension of tensor_matrix to fill them with node features for each subject\n",
    "    for i in range(num_subjects):\n",
    "        # Set edges for the graph as in tensor_matrix\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "        for j in range(num_nodes):\n",
    "            for k in range(j+1, num_nodes):\n",
    "                weight = tensor_matrix[j, k, i]\n",
    "                # Remove null edges? (Yang doesn't)  \n",
    "                # Add both directions since undirected?\n",
    "                if weight != 0:  # Remove null edges\n",
    "                    edges.append([j, k])\n",
    "                    edges.append([k, j])\n",
    "                    edge_weights.append(weight)\n",
    "                    edge_weights.append(weight)\n",
    "\n",
    "        # Convert graph edges in form of torch tensor of size (2, num_edges)\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() \n",
    "\n",
    "        # Convert graph edge weights in form of torch tensor \n",
    "        edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "\n",
    "        # Create graph data object\n",
    "        data = Data(x=feature_tensor, edge_index=edge_index, edge_attr=edge_weights)\n",
    "        \n",
    "        # Append the graph data object to the list\n",
    "        graph_list.append(data)\n",
    "\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "# Calculate node features for both structural and functional matrices\n",
    "features = {\n",
    "    'sc_ya': calculate_node_features(matrices['sc_ya']),\n",
    "    'sc_oa': calculate_node_features(matrices['sc_oa']),\n",
    "    'fc_ya': calculate_node_features(matrices['fc_ya']),\n",
    "    'fc_oa': calculate_node_features(matrices['fc_oa']),\n",
    "    'combined_ya': torch.cat((calculate_node_features(matrices['sc_ya']), calculate_node_features(matrices['fc_ya'])), dim=1),\n",
    "    'combined_oa': torch.cat((calculate_node_features(matrices['sc_oa']), calculate_node_features(matrices['fc_oa'])), dim=1),\n",
    "}\n",
    "\n",
    "\n",
    "# Each element is a list of graph data objects\n",
    "#TODO: stack functional and structural features together?\n",
    "#TODO: using multi-edge connections instead of separate modalities?\n",
    "combined_graphs = {\n",
    "    'sc_ya': combined_graph(matrices['sc_ya'], feature_tensor=features['sc_ya']),\n",
    "    'sc_oa': combined_graph(matrices['sc_oa'], feature_tensor=features['sc_oa']),\n",
    "    #'fc_ya': combined_graph(matrices['fc_ya'], feature_tensor=features['fc_ya']),\n",
    "    #'fc_oa': combined_graph(matrices['fc_oa'], feature_tensor=features['fc_oa']),\n",
    "    'fc_sc_ya': combined_graph(matrices['fc_ya'], feature_tensor=features['combined_ya']),\n",
    "    'fc_sc_oa': combined_graph(matrices['fc_oa'], feature_tensor=features['combined_oa']),\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the len of the combined graphs\n",
    "\n",
    "\n",
    "graphs_sc = combined_graphs['sc_ya']+combined_graphs['sc_oa']\n",
    "labels_sc = [0]*len(combined_graphs['sc_ya']) + [1]*len(combined_graphs['sc_oa'])\n",
    "#dataset_sc = list(zip(graphs_sc, labels_sc))\n",
    "\n",
    "\n",
    "graphs_fc_sc = combined_graphs['fc_sc_ya']+combined_graphs['fc_sc_oa']\n",
    "labels_fc_sc = [0]*len(combined_graphs['fc_sc_ya']) + [1]*len(combined_graphs['fc_sc_oa'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_sc len: 125, type: <class 'list'>\n",
      "test_data_sc len: 54, type: <class 'list'>\n",
      "train_labels_sc len: 125, type: <class 'list'>\n",
      "test_labels_sc len: 54, type: <class 'list'>\n",
      "train_data_sc[0] shape: torch.Size([100, 4])\n",
      "train_data_sc[0] edge_index shape: torch.Size([2, 9900])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data with stratification\n",
    "train_data_sc, test_data_sc, train_labels_sc, test_labels_sc = train_test_split(graphs_sc, labels_sc, test_size=0.3, random_state=42, stratify=labels_sc)\n",
    "#train_data_fc_sc, test_data_fc_sc, train_labels_fc_sc, test_labels_fc_sc = train_test_split(graphs_fc_sc, labels_fc_sc, test_size=0.3, random_state=42, stratify=labels_fc_sc)\n",
    "# train_data_fc, test_data_fc, train_labels_fc, test_labels_fc = train_test_split(graphs_fc, labels_fc, test_size=0.3, random_state=42, stratify=labels_fc)\n",
    "\n",
    "# Print the shapes of the split data\n",
    "\n",
    "print(f'train_data_sc len: {len(train_data_sc)}, type: {type(train_data_sc)}') \n",
    "print(f'test_data_sc len: {len(test_data_sc)}, type: {type(test_data_sc)}')\n",
    "print(f'train_labels_sc len: {len(train_labels_sc)}, type: {type(train_labels_sc)}')\n",
    "print(f'test_labels_sc len: {len(test_labels_sc)}, type: {type(test_labels_sc)}')\n",
    "\n",
    "\n",
    "# print the shape of the first element in the train_data_sc\n",
    "print(f'train_data_sc[0] shape: {train_data_sc[0].x.shape}')\n",
    "print(f'train_data_sc[0] edge_index shape: {train_data_sc[0].edge_index.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset_sc len: 125\n",
      "test_dataset_sc len: 54\n",
      "graph shape: torch.Size([100, 4]), label: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch dataset as a subclass of torch.utils.data.Dataset\n",
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs, labels):\n",
    "        self.graphs = graphs # list of graphs (Data objects)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return graph, label\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset_sc = GraphDataset(train_data_sc, train_labels_sc)\n",
    "test_dataset_sc = GraphDataset(test_data_sc, test_labels_sc)\n",
    "\n",
    "\n",
    "#train_dataset_fc_sc = GraphDataset(train_data_fc_sc, train_labels_fc_sc)\n",
    "#test_dataset_fc_sc = GraphDataset(test_data_fc_sc, test_labels_fc_sc)\n",
    "\n",
    "# train_dataset_fc = GraphDataset(train_data_fc, train_labels_fc)\n",
    "# test_dataset_fc = GraphDataset(test_data_fc, test_labels_fc)\n",
    "\n",
    "\n",
    "# Print the length of the datasets\n",
    "\n",
    "print(f'train_dataset_sc len: {len(train_dataset_sc)}')\n",
    "print(f'test_dataset_sc len: {len(test_dataset_sc)}')\n",
    "\n",
    "# print the first element of the train_dataset_sc\n",
    "graph, label = train_dataset_sc[0]\n",
    "print(f'graph shape: {graph.x.shape}, label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader_sc len: 4\n",
      "data len: 32, labels len: 32\n",
      "data[0] shape: torch.Size([100, 4]), labels[0]: 0\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_sc = DataLoader(train_dataset_sc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_sc = DataLoader(test_dataset_sc, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "train_loader_fc_sc = DataLoader(train_dataset_fc_sc, batch_size=batch_size, shuffle=True)\n",
    "test_loader_fc_sc = DataLoader(test_dataset_fc_sc, batch_size=batch_size, shuffle=False)\n",
    "\"\"\"\n",
    "# train_loader_fc = DataLoader(train_dataset_fc, batch_size=batch_size, shuffle=True)\n",
    "# test_loader_fc = DataLoader(test_dataset_fc, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(f'train_loader_sc len: {len(train_loader_sc)}')\n",
    "\n",
    "# print the length of the first element in the train_loader_sc\n",
    "for data, labels in train_loader_sc:\n",
    "    print(f'data len: {len(data)}, labels len: {len(labels)}')\n",
    "    print(f'data[0] shape: {data[0].x.shape}, labels[0]: {labels[0]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# GAT model\n",
    "# Interpretable graph classification\n",
    "# Captum for interpretability?\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gat.py\n",
    "\n",
    "#--------------------------------\n",
    "# Device configuration\n",
    "#--------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: %s' % device)\n",
    "\n",
    "#--------------------------------\n",
    "# Hyper-parameters\n",
    "#--------------------------------\n",
    "in_channels = 4\n",
    "out_channels = 8\n",
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 1e-1\n",
    "learning_rate_decay = 0.95\n",
    "reg = 0.001\n",
    "num_training = 125\n",
    "num_test = 54\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=1):  # hidden_channels\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=heads, concat=True, dropout=0.6)\n",
    "        self.classifier = nn.Linear(out_channels * heads, num_classes)  # Adjust output size based on concatenation\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)  # x = F.elu(x)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = self.classifier(x)\n",
    "        return x  # shape = (num_nodes, num_classes)\n",
    "\n",
    "# Initialize model, criterion and optimizer\n",
    "model = GAT(in_channels=in_channels, out_channels=out_channels, heads=1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Training function\n",
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward pass\n",
    "        out = model(data)\n",
    "        # compute loss\n",
    "        loss = criterion(out, labels)\n",
    "        # zero the gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # accumulate loss\n",
    "        total_loss += loss.item()\n",
    "    # average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, labels in loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.9565, Test Accuracy: 55.56%\n",
      "Epoch [2/20], Loss: 0.6823, Test Accuracy: 44.44%\n",
      "Epoch [3/20], Loss: 0.6692, Test Accuracy: 55.56%\n",
      "Epoch [4/20], Loss: 0.6901, Test Accuracy: 55.56%\n",
      "Epoch [5/20], Loss: 0.6769, Test Accuracy: 55.56%\n",
      "Epoch [6/20], Loss: 0.6821, Test Accuracy: 55.56%\n",
      "Epoch [7/20], Loss: 0.6826, Test Accuracy: 55.56%\n",
      "Epoch [8/20], Loss: 0.6768, Test Accuracy: 55.56%\n",
      "Epoch [9/20], Loss: 0.6562, Test Accuracy: 55.56%\n",
      "Epoch [10/20], Loss: 0.6599, Test Accuracy: 55.56%\n",
      "Epoch [11/20], Loss: 0.6491, Test Accuracy: 55.56%\n",
      "Epoch [12/20], Loss: 0.6442, Test Accuracy: 55.56%\n",
      "Epoch [13/20], Loss: 0.5457, Test Accuracy: 100.00%\n",
      "Epoch [14/20], Loss: 0.5429, Test Accuracy: 55.56%\n",
      "Epoch [15/20], Loss: 0.5304, Test Accuracy: 100.00%\n",
      "Epoch [16/20], Loss: 0.4241, Test Accuracy: 100.00%\n",
      "Epoch [17/20], Loss: 0.4178, Test Accuracy: 55.56%\n",
      "Epoch [18/20], Loss: 0.3710, Test Accuracy: 100.00%\n",
      "Epoch [19/20], Loss: 0.3326, Test Accuracy: 100.00%\n",
      "Epoch [20/20], Loss: 0.3103, Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Epoch loop with prints\n",
    "lr = learning_rate  # Initialize learning rate\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(train_loader_sc)\n",
    "    test_accuracy = test(test_loader_sc)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "    # Code to update the lr\n",
    "    lr *= learning_rate_decay\n",
    "    update_lr(optimizer, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "# This website has got some awesome visualizations check it out:\n",
    "# http://networkrepository.com/graphvis.php?d=./data/gsm50/labeled/cora.edges\n",
    "\n",
    "#https://towardsdatascience.com/large-graph-visualization-tools-and-approaches-2b8758a1cd59\n",
    "\n",
    "# igraph "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
